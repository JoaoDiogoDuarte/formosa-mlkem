#ifndef POLY_INVNTT_HH
#define POLY_INVNTT_HH

#include "params.jahh"
#include "reduce.jahh"
#include "consts.jahh"

inline
fn invntt_butterfly64x(reg u256 rl0 rl1 rl2 rl3 rh0 rh1 rh2 rh3 zl0 zl1 zh0 zh1 qx16) 
    -> reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256
{
  reg u256 t0 t1 t2 t3 t4 t5 t6 t7;

  t0  = #VPSUB_16u16(rl0, rh0);
  t1  = #VPSUB_16u16(rl1, rh1);
  t2  = #VPSUB_16u16(rl2, rh2);

  rl0 = #VPADD_16u16(rh0, rl0);
  rl1 = #VPADD_16u16(rh1, rl1);
  rh0 = #VPMULL_16u16(zl0, t0);

  rl2 = #VPADD_16u16(rh2, rl2);
  rh1 = #VPMULL_16u16(zl0, t1);
  t3  = #VPSUB_16u16(rl3, rh3);

  rl3 = #VPADD_16u16(rh3, rl3);
  rh2 = #VPMULL_16u16(zl1, t2);
  rh3 = #VPMULL_16u16(zl1, t3);
  
  t0  = #VPMULH_16u16(zh0, t0);
  t1  = #VPMULH_16u16(zh0, t1);

  t2  = #VPMULH_16u16(zh1, t2);
  t3  = #VPMULH_16u16(zh1, t3);

  // Reduce
  rh0  = #VPMULH_16u16(qx16, rh0);
  rh1  = #VPMULH_16u16(qx16, rh1);
  rh2  = #VPMULH_16u16(qx16, rh2);
  rh3  = #VPMULH_16u16(qx16, rh3);
  
  rh0  = #VPSUB_16u16(t0, rh0);
  rh1  = #VPSUB_16u16(t1, rh1);
  rh2  = #VPSUB_16u16(t2, rh2);
  rh3  = #VPSUB_16u16(t3, rh3);

  return rl0, rl1, rl2, rl3, rh0, rh1, rh2, rh3;
}


inline fn invntt_level6(reg ptr u16[KYBER_N] rp) -> reg ptr u16[KYBER_N]
{
  reg u256 zeta0 zeta1 r0 r1 r2 r3 r4 r5 r6 r7 qx16 flox16 fhix16;
  reg u32 t;
  reg u16 w;
  reg ptr u16[400] zetasp;
  reg ptr u16[16] qx16p;
  inline int i;

  zetasp = jzetas_inv_exp;
  qx16p = jqx16;

  qx16 = jqx16[u256 0];

  zeta0 = #VPBROADCAST_8u32(zetasp.[u32 784]);
  zeta1 = #VPBROADCAST_8u32(zetasp.[u32 788]);

  for i=0 to 2
  {
    r0 = rp.[u256 32*0+128*i];
    r1 = rp.[u256 32*1+128*i];
    r2 = rp.[u256 32*2+128*i];
    r3 = rp.[u256 32*3+128*i];
    r4 = rp.[u256 32*8+128*i];
    r5 = rp.[u256 32*9+128*i];
    r6 = rp.[u256 32*10+128*i];
    r7 = rp.[u256 32*11+128*i];

    r0, r1, r2, r3, r4, r5, r6, r7 = invntt_butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    flox16 = jflox16[u256 0];
    fhix16 = jfhix16[u256 0];

    rp.[u256 32*8+128*i]  = r4;
    rp.[u256 32*9+128*i]  = r5;
    rp.[u256 32*10+128*i] = r6;
    rp.[u256 32*11+128*i] = r7;

    r0 = fqmulprecomp16x(r0, flox16, fhix16, qx16);
    r1 = fqmulprecomp16x(r1, flox16, fhix16, qx16);
    r2 = fqmulprecomp16x(r2, flox16, fhix16, qx16);
    r3 = fqmulprecomp16x(r3, flox16, fhix16, qx16);

    rp.[u256 32*0+128*i] = r0;
    rp.[u256 32*1+128*i] = r1;
    rp.[u256 32*2+128*i] = r2;
    rp.[u256 32*3+128*i] = r3;
  }

  return rp;
}



fn poly_invntt(reg ptr u16[KYBER_N] rp) -> reg ptr u16[KYBER_N]
{
  reg u64 len;  
  reg u64 start;
  reg u64 j;
  reg u64 cmp;
  reg u64 offset;
  
  reg u16 zeta;
  reg u16 t;
  reg u16 s;
  reg u16 m;

  reg ptr u16[128] zetasp;
  reg u64 zetasctr;

  zetasp = jzetas_inv;
  zetasctr = 0;

  len = 2;
  while (len <= 128)
  {
    start = 0;
    while (start < 256)
    {
      zeta = zetasp[(int)zetasctr];
      zetasctr += 1;

      j = start;
      cmp = start + len;
      while (j < cmp)
      {
        offset = j + len;
        s = rp[(int)offset];
        t = rp[(int)j];
        m = s + t;
        m = barrett_reduce(m);
        rp[(int)j] = m;
        t -= s;
        t = fqmul(t, zeta);
        rp[(int)offset] = t;
        j += 1;
      }
      start = j + len;
    }
    len <<= 1;
  }

  zeta = zetasp[127];
  j = 0;
  while (j < KYBER_N) 
  {
    t = rp[(int)j];
    t = fqmul(t, zeta);
    rp[(int)j] = t;
    j += 1;
  }
  return rp;
}

#endif
