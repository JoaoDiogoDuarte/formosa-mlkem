#ifndef POLYVEC_COMPRESS_HH
#define POLYVEC_COMPRESS_HH

#include "params.jahh"
#include "polyvec_csubq.jahh"


u16 pvc_off_s = 0x0f;
u16 pvc_shift1_s = 0x1000;
u16 pvc_mask_s = 0x03ff;
u64 pvc_shift2_s = 0x0400000104000001;
u64 pvc_sllvdidx_s = 0x0C;
u8[32] pvc_shufbidx_s = {0, 1, 2, 3, 4, 8, 9, 10, 11, 12, -1, -1, -1, -1, -1, -1,
                         9, 10, 11, 12, -1, -1, -1, -1, -1, -1, 0, 1, 2, 3, 4, 8};

inline
fn polyvec_compress(reg u64 rp, stack u16[KYBER_VECN] a)
{
  inline int i;
  reg u256 f0 f1 f2 v v8 off shift1 mask shift2 sllvdidx shufbidx;
  reg u128 t0 t1;
  reg ptr u16[16] x16p;
  reg ptr u8[32] x8p;

  a = polyvec_csubq(a);

  x16p = jvx16;
  v = x16p[u256 0];
  v8 = #VPSLL_16u16(v, 3);
  off = #VPBROADCAST_16u16(pvc_off_s);
  shift1 = #VPBROADCAST_16u16(pvc_shift1_s);
  mask = #VPBROADCAST_16u16(pvc_mask_s);
  shift2 = #VPBROADCAST_4u64(pvc_shift2_s);
  sllvdidx = #VPBROADCAST_4u64(pvc_sllvdidx_s);
  shufbidx = pvc_shufbidx_s[u256 0];

  for i=0 to KYBER_VECN/16
  {
    f0 = a[u256 i];
    f1 = #VPMULL_16u16(f0, v8);
    f2 = #VPADD_16u16(f0, off);
    f0 = #VPSLL_16u16(f0, 3);
    f0 = #VPMULH_16u16(f0, v);
    f2 = #VPSUB_16u16(f1, f2);
    f1 = #VPANDN_256(f1, f2);
    f1 = #VPSRL_16u16(f1, 15);
    f0 = #VPSUB_16u16(f0, f1);
    f0 = #VPMULHRS_16u16(f0, shift1);
    f0 = #VPAND_256(f0, mask);
    f0 = #VPMADDWD_256(f0, shift2);
    f0 = #VPSLLV_8u32(f0, sllvdidx);
    f0 = #VPSRL_4u64(f0, 12);
    f0 = #VPSHUFB_256(f0, shufbidx);
    t0 = (128u)f0;
    t1 = #VEXTRACTI128(f0, 1);
    t0 = #VPBLEND_8u16(t0, t1, 0xE0);
    (u128)[rp + 20*i] = t0;
    (u32)[rp + 20*i + 16] = #VPEXTR_32(t1, 0);
  }
}

inline
fn i_polyvec_compress(reg ptr u8[KYBER_POLYVECCOMPRESSEDBYTES] rp, stack u16[KYBER_VECN] a) -> reg ptr u8[KYBER_POLYVECCOMPRESSEDBYTES]
{
  inline int i;
  reg u256 f0 f1 f2 v v8 off shift1 mask shift2 sllvdidx shufbidx;
  reg u128 t0 t1;
  reg ptr u16[16] x16p;
  reg ptr u8[32] x8p;

  a = polyvec_csubq(a);

  x16p = jvx16;
  v = x16p[u256 0];
  v8 = #VPSLL_16u16(v, 3);
  off = #VPBROADCAST_16u16(pvc_off_s);
  shift1 = #VPBROADCAST_16u16(pvc_shift1_s);
  mask = #VPBROADCAST_16u16(pvc_mask_s);
  shift2 = #VPBROADCAST_4u64(pvc_shift2_s);
  sllvdidx = #VPBROADCAST_4u64(pvc_sllvdidx_s);
  shufbidx = pvc_shufbidx_s[u256 0];

  for i=0 to KYBER_VECN/16
  {
    f0 = a[u256 i];
    f1 = #VPMULL_16u16(f0, v8);
    f2 = #VPADD_16u16(f0, off);
    f0 = #VPSLL_16u16(f0, 3);
    f0 = #VPMULH_16u16(f0, v);
    f2 = #VPSUB_16u16(f1, f2);
    f1 = #VPANDN_256(f1, f2);
    f1 = #VPSRL_16u16(f1, 15);
    f0 = #VPSUB_16u16(f0, f1);
    f0 = #VPMULHRS_16u16(f0, shift1);
    f0 = #VPAND_256(f0, mask);
    f0 = #VPMADDWD_256(f0, shift2);
    f0 = #VPSLLV_8u32(f0, sllvdidx);
    f0 = #VPSRL_4u64(f0, 12);
    f0 = #VPSHUFB_256(f0, shufbidx);
    t0 = (128u)f0;
    t1 = #VEXTRACTI128(f0, 1);
    t0 = #VPBLEND_8u16(t0, t1, 0xE0);
    rp.[u128 20*i] = t0;
    rp.[u32 20*i + 16] = #VPEXTR_32(t1, 0);
  }

  return rp;
}

#endif
